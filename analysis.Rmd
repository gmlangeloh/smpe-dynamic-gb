---
title: "Evaluating dynamic Gröbner Basis algorithms"
author: "Gabriel Mattos Langeloh"
date: "18/11/2019"
output: html_document
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE)
library(tidyverse)

dyn_results <- read.table("data/tiamat0-results5.out", header=T)
rand_results <- read.table("data/tiamat0-random.out", header=T)
ls_results <- read.table("data/tiamat2-ls.out", header=T)
dyn_results <- rbind(dyn_results, rand_results, ls_results)
```

# Introduction

The goal of this document is to do some exploratory analysis on dynamic Gröbner Basis algorithms. *Gröbner bases* 
(GB) are a fundamental tool in computer algebra to solve multivariate polynomial (non-linear) systems, among other
applications. Traditional *static* Gröbner Basis algorithms receive an ordering (a vector) as part of the
input. The performance of the algorithm, as well as the size of the output itself strongly depends on this input
ordering. *Dynamic* Gröbner Basis algorithms were proposed to avoid having to choose an ordering a priori,
computing one during the execution of the algorithm itself, hoping that this would lead to at least some
of the following:

- shorter computation times
- fewer polynomials in the output
- sparser polynomials in the output
- polynomials of lower degree in the output

These are the main values that are computed in the experiments that follow. Six algorithms were used: 

- the static algorithm (with a fixed input ordering, commonly used in the literature)
- the Gritzmann-Sturmfels algorithm (proposed in [@Gritzmann1993], essentially a brute force search)
- the caboara-perry algorithm (proposed in [@Caboara2014], uses linear programming)
- the perturbation algorithm (new, looks for better orderings by applying perturbations to the current one)
- the random algorithm (new, does a random walk in the space of orderings)
- the local search algorithm (new, does a local search over the search space)

Due to particular traits of these algorithms, the perturbation and random algorithms only look for new orderings
every 10 iterations of the Gröbner basis computation. It would also be interesting to find a better period
more rigorously, but we do not do that here.

# Experimental setup and environment

All algorithms were implemented in the Sage computer algebra system (version 8.8, Python version 3.7.3) 
and share all basic functionality from the underlying algorithm used to compute Gröbner Bases.
Our implementation is based on that of [@Caboara2014].
Experiments were run on an AMD FX(tm)-8150 Eight-Core Processor @ 3.60GHz machine with 32GB of RAM.

All `r length(unique(dyn_results$instance))` instances were extracted from 
[Christian Eder's benchmark library](https://github.com/ederc/singular-benchmarks) for 
Gröbner Bases. Each algorithm was run on every instance 30 times, the presented results corresponding to the
mean of these runs. We used time limits of 60 minutes, and NA results correspond to timeouts.

Average results between groups of instances use the geometric mean, as some instances are much larger than others.

```{r, eval=FALSE, include=FALSE}
"Instances"
unique(dyn_results$instance)
"Algorithms"
unique(dyn_results$algorithm)
```

```{r, include=FALSE}
by_instance <- dyn_results %>% group_by(instance)
by_algorithm <- dyn_results %>% group_by(algorithm)
by_inst_alg <- dyn_results %>% 
  group_by(instance, algorithm, reducer) %>% 
  summarize_all(mean, na.rm=TRUE) %>%
  select(-rep)
buchberger <- by_inst_alg %>% filter(reducer == 'classical')
f4 <- by_inst_alg %>% filter(reducer == 'F4')
```

# Partial results from some previous works

The following table shows the results reported in [@Caboara2014], compared to our reproduction. 
Timings are not reported in the paper. 
We cannot reproduce these results, even using the code supplied in the original paper. 

```{r, warning=FALSE}
caboara_perry <- read.table("data/caboara-perry2014.out", header = TRUE)

repr_cp <- by_inst_alg %>%
  filter(algorithm=='caboara-perry', reducer=='classical') %>%
  ungroup %>%
  select(instance, polynomials, monomials) %>%
  filter(instance %in% caboara_perry$instance)

how_reproducible <- inner_join(caboara_perry, repr_cp, by="instance") %>%
  rename(
    "Original polynomials" = polynomials.x,
    "Original monomials" = monomials.x,
    "Our polynomials" = polynomials.y,
    "Our monomials" = monomials.y
  )


knitr::kable(
  how_reproducible,
  caption="Attempt to reproduce results from Caboara and Perry (2014)"
)
```

There are multiple possibilities for why we could not reproduce the results of the original paper. One is that their
code may have changed between the publication of the paper and our access. As they did not provide the instances,
only their names (they are all classical benchmarks in the literature) there may be slight differences in the
instances themselves. Each instance is a list of polynomials - they may have been provided in a different order,
leading to different results. Also, one of the properties of the instances (their characteristic) was not reported in
the paper, so we had to choose an arbitrary value common in the literature.

For completeness, we also show the results from [@Perry2017], that uses a slightly modified version of the 
caboara-perry algorithm and a simple implementation in C++.

```{r, warning=FALSE}
perry <- read.table("data/perry2017.out", header = TRUE)
comp_cp <- by_inst_alg %>%
  filter(algorithm=='caboara-perry', reducer=='classical') %>%
  ungroup %>%
  select(instance, polynomials, monomials, time) %>%
  filter(instance %in% perry$instance)
knitr::kable(inner_join(perry, comp_cp, by="instance") %>%
  rename(
    "Their polynomials" = polynomials.x,
    "Their monomials" = monomials.x,
    "Their time" = time.x,
    "Our polynomials" = polynomials.y,
    "Our monomials" = monomials.y,
    "Our time" = time.y
  ),
  caption="Comparison with the results of Perry (2017)"
  )
```

Our implementation is much slower, but that is expected, as the implementation from [@Perry2017] is in C++ and ours
is in Sage / Python. It is interesting to note, however, that for most instances their implementation is faster by almost a factor of 10. This is not the case of cyclicnh7, where it is only about 2 times faster. This points to an algorithmic advantage of the caboara-perry algorithm in this case.

# Exploratory analysis

```{r, include=FALSE}
#We drop zeroes in the geometric mean, it is important in some cases with overhead 0
gmean <- function(x, na.rm=FALSE) {
  y <- x[which(x != 0)]
  if (length(y) == 0) {
    return(0)
  }
  return(exp(mean(log(y), na.rm=na.rm)))
}
```

## Classical reduction vs F4

Two reducers (a component of the general Gröbner Basis algorithm) were implemented: classical (based on polynomial 
arithmetic) and F4 (introduced in [@faugereF4], uses matrix row reduction). F4 is usually considered to be faster
in the context of traditional static Gröbner Basis algorithms. We observe this as well in our implementation, as shown by the table below.

```{r}
knitr::kable(
by_inst_alg %>%
  filter(algorithm=='static') %>%
  group_by(reducer) %>%
  summarize(time=gmean(time, na.rm=TRUE)),
caption="Geometric means of the Static algorithm over all instances, by reducer."
)
```

Is the speedup of using the F4 reducer kept for dynamic algorithms? In average, yes. The geometric mean of the running time, over all algorithms using the classical reducer is `r gmean(buchberger$time, na.rm=TRUE)` while when we use the F4 reducer, we get `r gmean(f4$time, na.rm=TRUE)`.

The speedup of the F4 reducer is kept for most, but not all, pairs of instances and algorithms. The following graphs show results instance by instance, for three algorithms (Caboara-Perry, Static, Perturbation).

```{r, warning=FALSE}
cp <- by_inst_alg %>% filter(algorithm == 'caboara-perry')
static <- by_inst_alg %>% filter(algorithm == 'static')
perturb <- by_inst_alg %>% filter(algorithm == 'perturbation')

ggplot(cp, aes(x=instance, y=1 + time, fill=reducer)) + 
  geom_col(position='dodge') + 
  coord_flip() + 
  scale_y_log10() +
  ylab("log(1 + time)") +
  ggtitle("Caboara-Perry algorithm")

ggplot(static, aes(x=instance, y=1 + time, fill=reducer)) + 
  geom_col(position='dodge') + 
  coord_flip() + 
  scale_y_log10() +
  ylab("log(1 + time)") +
  ggtitle("Static algorithm")

ggplot(perturb, aes(x=instance, y=1 + time, fill=reducer)) + 
  geom_col(position='dodge') + 
  coord_flip() + 
  scale_y_log10() +
  ylab("log(1 + time)") +
  ggtitle("Perturbation algorithm")
```

Note, further, that some running times do not appear in these graphs. In these cases, timeouts occurred. In a few cases, using
the F4 reducer led to a timeout when the classical reducer did not.

## Comparing the dynamic Gröbner Basis algorithms

At first we focus on the five more viable algorithms (all but algorithm GS of Gritzmann and Sturmfels, as it is too slow to terminate
in the time limit for most instances). To summarize the results, we compute the geometric mean of running time, the overhead
of the dynamic components (time spent in the functions related to dynamic algorithms), number of polynomials and monomials of the
output, and maximum degree of a polynomial in the output. The column S-reductions roughly corresponds to the number of iterations
in the main loop of the algorithm.

```{r}
knitr::kable(
  by_inst_alg %>% 
  group_by(algorithm, reducer) %>%
  summarize(
    timeout=sum(is.na(time)),
    time=gmean(time, na.rm=TRUE),
    overhead=gmean(dynamic, na.rm=TRUE),
    polynomials=gmean(polynomials, na.rm=TRUE),
    monomials=gmean(monomials, na.rm=TRUE),
    degree=gmean(degree, na.rm=TRUE),
    sreductions=gmean(sreductions, na.rm=TRUE)
  ) %>%
  select(
    algorithm, reducer, time, overhead, polynomials, 
    monomials, degree, sreductions,timeout
  ),
  caption="Geometric means of each algorithm and reducer over all instances over which they did not timeout."
)
```

We first remark that most of these results cannot be compared directly, due to the timeouts. Still, we can see that Perturbation, Random and Static lead to bases of smaller degree than Caboara-Perry and LocalSearch. Overall, the LocalSearch algorithm performs
poorly, with many timeouts, very high overhead and high degree. The main reason its number of polynomials and monomials is so low is
because it times out in the biggest instances. Further evidence for this is in the tables below, which show this algorithm leads to small bases, but it does not perform better in this sense than Caboara-Perry or Perturbation.

```{r}
knitr::kable(buchberger %>%
  group_by(instance) %>%
  filter(dense_rank(polynomials) == 1) %>%
  group_by(algorithm) %>%
  summarize("% fewest polynomials"=n() / 30),
  caption="Proportion of instances where each algorithm found the basis with fewest polynomials, classical reducer."
)
knitr::kable(buchberger %>%
  group_by(instance) %>%
  filter(dense_rank(degree) == 1) %>%
  group_by(algorithm) %>%
  summarize("%lowest degree"=n() / 30),
  caption="Proportion of instances where each algorithm found the basis with lowest degree, classical reducer."
)
knitr::kable(buchberger %>%
  group_by(instance) %>%
  filter(dense_rank(time) == 1) %>%
  group_by(algorithm) %>%
  summarize("% fastest"=n() / 30),
  caption="Proportion of instances where each algorithm terminated more quickly, classical reducer."
)
```

```{r}
knitr::kable(f4 %>%
  group_by(instance) %>%
  filter(dense_rank(polynomials) == 1) %>%
  group_by(algorithm) %>%
  summarize("% fewest polynomials"=n() / 30),
  caption="Proportion of instances where each algorithm found the basis with fewest polynomials, F4 reducer."
)
knitr::kable(f4 %>%
  group_by(instance) %>%
  filter(dense_rank(degree) == 1) %>%
  group_by(algorithm) %>%
  summarize("% lowest degree"=n() / 30),
  caption="Proportion of instances where each algorithm found the basis with lowest degree, F4 reducer."
)
knitr::kable(f4 %>%
  group_by(instance) %>%
  filter(dense_rank(time) == 1) %>%
  group_by(algorithm) %>%
  summarize("% fastest" =n() / 30),
  caption="Proportion of instances where each algorithm terminated more quickly, F4 reducer."
)
```

It is somewhat remarkable that the Random algorithm is able to find bases almost as small as Caboara-Perry and Perturbation, given
that it simply visits random solutions. Even so, it seems to have no advantages when compared to Perturbation, as it is slower,
and Perturbation is also very simple.

The Perturbation algorithm is faster than Caboara-Perry on most instances, due to its lower overhead, and it also leads to bases
of lower degree. This is important - it is known that, asymptotically, large degrees lead to much worse running times, which gives
evidence to the claim that Perturbation scales better than Caboara-Perry even though it returns slightly more polynomials.

The traditional Static algorithm still outperforms dynamic ones in terms of running time, unfortunately. Due to the overhead
of the dynamic algorithms, this is always the case except when the dynamic algorithms manage to terminate in much fewer iterations. One such case is
the homogeneous Cyclic family of instances. This is shown in the next table.

```{r}
c67h <- by_inst_alg %>%
  filter(instance == 'cyclicnh7' | instance == 'cyclicnh6') %>%
  select(instance, algorithm, reducer, time, dynamic, polynomials, monomials, degree, sreductions) %>%
  ungroup()

c8h_random <- read.table("data/tiamat0-random-c8h.out", header=T)
c8h_random <- c8h_random %>%
  group_by(instance, algorithm, reducer) %>%
  summarize_all(mean, na.rm=TRUE) %>%
  select(instance, algorithm, reducer, time, dynamic, polynomials, monomials, degree, sreductions) %>%
  ungroup()

c8h <- read.table("data/tiamat0-cyclicnh8.out", header=T)
c8h <- c8h %>%
  filter(algorithm != "random") %>%
  select(instance, algorithm, reducer, time, dynamic, polynomials, monomials, degree, sreductions)

big_cyclics <- rbind(c67h, c8h, c8h_random) %>%
  filter(algorithm %in% c("static", "caboara-perry", "perturbation"))
knitr::kable(
  big_cyclics,
  caption="Behavior of the algorithms on a few larger instances."
)
```

Note that, for cyclicnh7, both Caboara-Perry and Perturbation are up to 4 times faster than the Static algorithm. In cyclicnh8, Perturbation is the fastest algorithm, returning fewer polynomials of lower degree than both Static and
Caboara-Perry. By that point, the lower overhead of Perturbation in relation to
Caboara-Perry starts to pay off, and its running time is much lower.

This is additional evidence that Perturbation may scale better out of the implemented algorithms. It is not possible to test this further, with the current implementation, because even larger instances take days to run. For this reason,
a more efficient implementation of the dynamic algorithms, based on state-of-the-art Gröbner bases open source implementations is needed.

## Relationships between variables 

Now we want to investigate the relationship between variables, such as running time and number of polynomials in the output.
To do this, we first scatterplot a few of these pairs of variables.

```{r, warning=FALSE}

ggplot(data=by_inst_alg, aes(x=polynomials, y=time, color=algorithm)) +
  geom_point() +
  scale_y_log10() +
  scale_x_log10() +
  xlab("log(Polynomials)") +
  ylab("log(Time)")

ggplot(data=by_inst_alg, aes(x=degree, y=time, color=algorithm)) +
  geom_point() +
  scale_y_log10() +
  scale_x_log10() +
  xlab("log(Degree)") +
  ylab("log(Time)")

ggplot(buchberger, aes(x=sreductions, y=time, color=algorithm)) +
  geom_point() + 
  xlab("log(sreductions)") + 
  ylab("log(time)") +
  scale_x_log10() + 
  scale_y_log10()
```

All three pairs seem (graphically) to be positively correlated.
We compute their correlation for each algorithm and reducer separately:

```{r}
knitr::kable(
  by_inst_alg %>%
  group_by(algorithm, reducer) %>%
  summarize(
    "cor(polys, time)" = cor(polynomials, time, use="complete.obs"),
    "cor(degree, time)" = cor(degree, time, use="complete.obs"),
    "cor(sred, time)" = cor(sreductions, time, use="complete.obs")
    ),
  caption="Correlation between some variables and running time for each algorithm and reducer."
)
```

We observe that the correlation between sreductions and time is very high for most algorithms. This is not surprising, the number of iterations of the algorithm
evidently predicts its running time. Most algorithms also have $> 0.8$ correlation between number of polynomials and running time. This is likely due to, in general,
easier instances admitting smaller outputs. The degree, however, has a much lower
correlation with the running time. This is contrary to the intuition from the
literature, where asymptotic running times are often proven by bounds on the
degree.

Interestingly, the results of the localsearch algorithm don't follow the pattern
of the remaining ones. This probably happens because the running time in this case
is completely dominated by the dynamic overhead, which is mostly independent from the remaining parameters.

## Exploring the search space and the GS algorithm

The Gritzmann-Sturmfels algorithm (GS for short) is essentially a brute force
on the search space, as it visits every available solution. It was proposed
mostly as a theoretical tool with some interesting properties, and was not
previously implemented. We implemented it, however, to check how close it is
to optimality in the instances where it does not time out, as in these cases
it is possible to compute the entire search space.

```{r}
gs <- read.table("data/tiamat0-gs.out", header=T)
gs <- gs %>%
  filter(!is.na(time)) %>%
  select(-rep)

instances <- unique(gs$instance)
#Make a table with the relevant GFan data: min size, min deg, maybe size of the Pareto curve?
rows <- lapply(instances, function(instance_name) {
  gfan <- read.table(paste0("./data/", instance_name, ".gf"), header=T)
  minpolys <- min(gfan$polynomials)
  mindeg <- min(gfan$degree)
  return(tibble(instance=instance_name, minG=minpolys, mindeg=mindeg))
})
df <- bind_rows(rows)
gs <- full_join(gs, df, by="instance") %>% 
  select(-reducer, -algorithm, -heuristic, -queue, -reduction, -zeroreductions)
knitr::kable(
  gs,
  caption="GS algorithm results whenever it did not time out, compared to optimal
  outputs. The columns minG and mindeg correspond to the minimal possible number of   polynomials and degree, respectively. These may not be attainable at the same      time."
)
```

The GS algorithm was able to find optimal solutions in 4 out of 7 cases, and in 1 of the remaining cases it is not possible to optimize the number of polynomials and degree simultaneously.

Below, we plot the search space of multiple small instances completely, along with
the results of the algorithms whenever they are available. This is not
the case for r4 and r4h, because they are random instances and not traditional
literature benchmarks.

```{r}
library(ggrepel)
explore_gf <- function(instance_name) {
  
  data <- by_inst_alg %>%
    filter(instance == instance_name, reducer == "classical")
  
  gf_data <- read.table(paste0("./data/", instance_name, ".gf"), header=T)
  ggplot() +
    geom_count(data=gf_data, aes(x=polynomials, y=degree)) + 
    geom_point(data=data, aes(x=polynomials, y=degree, color=algorithm), size=3, show.legend=FALSE) +
    ggtitle(instance_name) +
    geom_text_repel(data=data, aes(x=polynomials, y=degree, color=algorithm, label=algorithm), show.legend=FALSE) + 
    scale_alpha(guide = 'none')
}

explore_gf("cyclicn4")
explore_gf("cyclicnh4")
explore_gf("cyclicn5")
explore_gf("katsuran4")
explore_gf("katsuranh4")
explore_gf("econ4")
explore_gf("econh4")
explore_gf("econ5")
explore_gf("r4")
explore_gf("r4h")
```

A simple interpretation of the quality of the output of the algorithms in these instances is that solutions closer to the bottom left are preferable. It is easy to observe that in many cases there is no solution minimizing both the degree and the
number of polynomials at the same time. This means the problem of finding the
"optimal" Gröbner Basis for an input is essentially a multi-objective optimization
problem. At the time, it is not clear how to design algorithms that prioritize
one over the other. 

Regardless of the multi-objective characteristics of the problem, we can see that there is margin for improvement in many of these instances, even though they
are small. Indeed, in many of them, there is an optimal solution and it is not found by any of the algorithms, or the solutions found are not in the Pareto curve of the instance (the Pareto curve of a multi-objective problem is the set of non-dominated solutions - a solution is dominated when some other solution is better than it with respect to every variable).

This multi-objective aspect was not previously observed in the literature - in fact, no previous work measures the degree of the output of the dynamic algorithms.

# Conclusions

- Currently, dynamic algorithms are not able to compete with traditional static ones except for larger instances.
- The new Perturbation algorithm is competitive with the previous best dynamic
algorithm, the Caboara-Perry algorithm, and seems to scale better than it. A
more efficient implementation is necessary to test this hypothesis more rigorously.
- There is a high correlation between time and sreductions and between time and
polynomials in the output. The correlation between time and degree is not nearly
as high, and that is somewhat surprising, when taking the theory of Gröbner Bases
into account.
- There is a multi-objective aspect to dynamic Gröbner Basis algorithms, as one
wants both outputs with low degree and small bases. We showed that often it is
not possible to minimize both at the same time, and the current algorithms are
not able to control what exactly they are minimizing.

# Bibliography